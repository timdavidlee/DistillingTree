{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class InnerNode():\n",
    "\n",
    "    def __init__(self, depth, args):\n",
    "        self.args = args\n",
    "        self.fc = nn.Linear(self.args.input_dim, 1)\n",
    "        beta = torch.randn(1)\n",
    "        #beta = beta.expand((self.args.batch_size, 1))\n",
    "        if self.args.cuda:\n",
    "            beta = beta.cuda()\n",
    "        self.beta = nn.Parameter(beta)\n",
    "        self.leaf = False\n",
    "        self.prob = None\n",
    "        self.leaf_accumulator = []\n",
    "        self.lmbda = self.args.lmbda * 2 ** (-depth)\n",
    "        self.build_child(depth)\n",
    "        self.penalties = []\n",
    "\n",
    "    def reset(self):\n",
    "        self.leaf_accumulator = []\n",
    "        self.penalties = []\n",
    "        self.left.reset()\n",
    "        self.right.reset()\n",
    "\n",
    "    def build_child(self, depth):\n",
    "        if depth < self.args.max_depth:\n",
    "            self.left = InnerNode(depth+1, self.args)\n",
    "            self.right = InnerNode(depth+1, self.args)\n",
    "        else :\n",
    "            self.left = LeafNode(self.args)\n",
    "            self.right = LeafNode(self.args)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return(F.sigmoid(self.beta*self.fc(x)))\n",
    "    \n",
    "    def select_next(self, x):\n",
    "        prob = self.forward(x)\n",
    "        if prob < 0.5:\n",
    "            return(self.left, prob)\n",
    "        else:\n",
    "            return(self.right, prob)\n",
    "\n",
    "    def cal_prob(self, x, path_prob):\n",
    "        self.prob = self.forward(x) #probability of selecting right node\n",
    "        self.path_prob = path_prob\n",
    "        left_leaf_accumulator = self.left.cal_prob(x, path_prob * (1-self.prob))\n",
    "        right_leaf_accumulator = self.right.cal_prob(x, path_prob * self.prob)\n",
    "        self.leaf_accumulator.extend(left_leaf_accumulator)\n",
    "        self.leaf_accumulator.extend(right_leaf_accumulator)\n",
    "        return(self.leaf_accumulator)\n",
    "\n",
    "    def get_penalty(self):\n",
    "        penalty = (torch.sum(self.prob * self.path_prob) / torch.sum(self.path_prob), self.lmbda)\n",
    "        self.penalties.append(penalty)\n",
    "        if not self.left.leaf:\n",
    "            left_penalty = self.left.get_penalty()\n",
    "            right_penalty = self.right.get_penalty()\n",
    "            \n",
    "            self.penalties.extend(left_penalty)\n",
    "            self.penalties.extend(right_penalty)\n",
    "        return(self.penalties)\n",
    "\n",
    "\n",
    "class LeafNode():\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.param = torch.randn(self.args.output_dim)\n",
    "        if self.args.cuda:\n",
    "            self.param = self.param.cuda()\n",
    "        self.param = nn.Parameter(self.param)\n",
    "        self.leaf = True\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self):\n",
    "        return(self.softmax(self.param.view(1,-1)))\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def cal_prob(self, x, path_prob):\n",
    "        Q = self.forward()\n",
    "        #Q = Q.expand((self.args.batch_size, self.args.output_dim))\n",
    "        Q = Q.expand((path_prob.size()[0], self.args.output_dim))\n",
    "        return([[path_prob, Q]])\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "class SoftDecisionTree(nn.Module):\n",
    "\n",
    "    def __init__(self, args):\n",
    "        super(SoftDecisionTree, self).__init__()\n",
    "        self.args = args\n",
    "        self.root = InnerNode(1, self.args)\n",
    "        self.collect_parameters() ##collect parameters and modules under root node\n",
    "        self.optimizer = optim.SGD(self.parameters(), lr=self.args.lr, momentum=self.args.momentum)\n",
    "        self.test_acc = []\n",
    "        self.define_extras(self.args.batch_size)\n",
    "        self.best_accuracy = 0.0\n",
    "\n",
    "    def define_extras(self, batch_size):\n",
    "        ##define target_onehot and path_prob_init batch size, because these need to be defined according to batch size, which can be differ\n",
    "        self.target_onehot = torch.FloatTensor(batch_size, self.args.output_dim)\n",
    "        self.target_onehot = Variable(self.target_onehot)\n",
    "        self.path_prob_init = Variable(torch.ones(batch_size, 1))\n",
    "        if self.args.cuda:\n",
    "            self.target_onehot = self.target_onehot.cuda()\n",
    "            self.path_prob_init = self.path_prob_init.cuda()\n",
    "    '''\n",
    "    def forward(self, x):\n",
    "        node = self.root\n",
    "        path_prob = Variable(torch.ones(self.args.batch_size, 1))\n",
    "        while not node.leaf:\n",
    "            node, prob = node.select_next(x)\n",
    "            path_prob *= prob\n",
    "        return node()\n",
    "    '''        \n",
    "    def cal_loss(self, x, y):\n",
    "        batch_size = y.size()[0]\n",
    "        leaf_accumulator = self.root.cal_prob(x, self.path_prob_init)\n",
    "        loss = 0.\n",
    "        max_prob = [-1. for _ in range(batch_size)]\n",
    "        max_Q = [torch.zeros(self.args.output_dim) for _ in range(batch_size)]\n",
    "        for (path_prob, Q) in leaf_accumulator:\n",
    "            TQ = torch.bmm(y.view(batch_size, 1, self.args.output_dim), torch.log(Q).view(batch_size, self.args.output_dim, 1)).view(-1,1)\n",
    "            loss += path_prob * TQ\n",
    "            path_prob_numpy = path_prob.cpu().data.numpy().reshape(-1)\n",
    "            for i in range(batch_size):\n",
    "                if max_prob[i] < path_prob_numpy[i]:\n",
    "                    max_prob[i] = path_prob_numpy[i]\n",
    "                    max_Q[i] = Q[i]\n",
    "        loss = loss.mean()\n",
    "        penalties = self.root.get_penalty()\n",
    "        C = 0.\n",
    "        for (penalty, lmbda) in penalties:\n",
    "            C -= lmbda * 0.5 *(torch.log(penalty) + torch.log(1-penalty))\n",
    "        output = torch.stack(max_Q)\n",
    "        self.root.reset() ##reset all stacked calculation\n",
    "        return(-loss + C, output) ## -log(loss) will always output non, because loss is always below zero. I suspect this is the mistake of the paper?\n",
    "\n",
    "    def collect_parameters(self):\n",
    "        nodes = [self.root]\n",
    "        self.module_list = nn.ModuleList()\n",
    "        self.param_list = nn.ParameterList()\n",
    "        while nodes:\n",
    "            node = nodes.pop(0)\n",
    "            if node.leaf:\n",
    "                param = node.param\n",
    "                self.param_list.append(param)\n",
    "            else:\n",
    "                fc = node.fc\n",
    "                beta = node.beta\n",
    "                nodes.append(node.right)\n",
    "                nodes.append(node.left)\n",
    "                self.param_list.append(beta)\n",
    "                self.module_list.append(fc)\n",
    "\n",
    "    def train_(self, train_loader, epoch):\n",
    "        self.train()\n",
    "        self.define_extras(self.args.batch_size)\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            correct = 0\n",
    "            if self.args.cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            #data = data.view(self.args.batch_size,-1)\n",
    "            target = Variable(target)\n",
    "            target_ = target.view(-1,1)\n",
    "            batch_size = target_.size()[0]\n",
    "            data = data.view(batch_size,-1)\n",
    "            ##convert int target to one-hot vector\n",
    "            data = Variable(data)\n",
    "            if not batch_size == self.args.batch_size: #because we have to initialize parameters for batch_size, tensor not matches with batch size cannot be trained\n",
    "                self.define_extras(batch_size)\n",
    "            self.target_onehot.data.zero_()            \n",
    "            self.target_onehot.scatter_(1, target_, 1.)\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            loss, output = self.cal_loss(data, self.target_onehot)\n",
    "            loss.backward(retain_variables=True)\n",
    "            self.optimizer.step()\n",
    "            pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.data).cpu().sum()\n",
    "            accuracy = 100. * correct / len(data)\n",
    "\n",
    "            if batch_idx % self.args.log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, Accuracy: {}/{} ({:.4f}%)'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.data[0],\n",
    "                    correct, len(data),\n",
    "                    accuracy))\n",
    "\n",
    "    def test_(self, test_loader, epoch):\n",
    "        self.eval()\n",
    "        self.define_extras(self.args.batch_size)\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        for data, target in test_loader:\n",
    "            if self.args.cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            target = Variable(target)\n",
    "            target_ = target.view(-1,1)\n",
    "            batch_size = target_.size()[0]\n",
    "            data = data.view(batch_size,-1)\n",
    "            ##convert int target to one-hot vector\n",
    "            data = Variable(data)\n",
    "            if not batch_size == self.args.batch_size: #because we have to initialize parameters for batch_size, tensor not matches with batch size cannot be trained\n",
    "                self.define_extras(batch_size)\n",
    "            self.target_onehot.data.zero_()            \n",
    "            self.target_onehot.scatter_(1, target_, 1.)\n",
    "            _, output = self.cal_loss(data, self.target_onehot)\n",
    "            pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.data).cpu().sum()\n",
    "        accuracy = 100. * correct / len(test_loader.dataset)\n",
    "        print('\\nTest set: Accuracy: {}/{} ({:.4f}%)\\n'.format(\n",
    "            correct, len(test_loader.dataset),\n",
    "            accuracy))\n",
    "        self.test_acc.append(accuracy)\n",
    "\n",
    "        if accuracy > self.best_accuracy:\n",
    "            self.save_best('./result')\n",
    "            self.best_accuracy = accuracy\n",
    "\n",
    "    def save_best(self, path):\n",
    "        try:\n",
    "            os.makedirs('./result')\n",
    "        except:\n",
    "            print('directory ./result already exists')\n",
    "\n",
    "        with open(os.path.join(path, 'best_model.pkl'), 'wb') as output_file:\n",
    "            pickle.dump(self, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "class myNestedImgDataset(Dataset):\n",
    "    def __init__(self, dir_path, transform=None, test=False):\n",
    "        self.dir_path = dir_path\n",
    "        self.transform = transform\n",
    "        self.classes = [x for x in os.listdir(dir_path) if os.path.isdir(os.path.join(dir_path,x))]\n",
    "        self.img_paths = []\n",
    "        self.labels = []\n",
    "        self.test = test\n",
    "        if self.test:\n",
    "            class_img_paths = [os.path.join(dir_path,x) for x in os.listdir(dir_path)]\n",
    "            self.img_paths.extend(class_img_paths)\n",
    "        else:\n",
    "            for class_idx, folder_name in enumerate(self.classes):\n",
    "                prefix = os.path.join(dir_path,folder_name)\n",
    "                class_img_paths = [os.path.join(prefix,x) for x in os.listdir(prefix)]\n",
    "                self.img_paths.extend(class_img_paths)\n",
    "                self.labels.extend(np.ones(len(class_img_paths))*class_idx)\n",
    "            \n",
    "            self.labels = [int(x) for x in self.labels]\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        if self.test:\n",
    "            return torch.FloatTensor(plt.imread(self.img_paths[idx])), None\n",
    "        else:\n",
    "            return torch.FloatTensor(plt.imread(self.img_paths[idx])), self.labels[idx]\n",
    "    \n",
    "    def show(self,idx):\n",
    "        return plt.imshow(mpimg.imread(self.img_paths[idx]), cmap='Greys')\n",
    "    \n",
    "\n",
    "class Img2FlatVec(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset= dataset\n",
    "        self.n = len(self.dataset)\n",
    "        samp_img, _ = self.dataset[1]\n",
    "        self.h, self.w = samp_img.shape\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        x, y = self.dataset[idx]\n",
    "        x_flat = x \n",
    "        return x.view(self.h*self.w)/255, y\n",
    "\n",
    "    def __len__(self): return self.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist_trn = myNestedImgDataset('/Users/timlee/data/MNIST/trn/')\n",
    "trn_vec = Img2FlatVec(mnist_trn)\n",
    "trn_dl = DataLoader(trn_vec, batch_size=8, shuffle=True, num_workers=4)\n",
    "x_test, y_test = iter(trn_dl).next()\n",
    "x_var = Variable(x_test)\n",
    "y_var = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory ./data already exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/timlee/anaconda2/envs/py3/lib/python3.6/site-packages/ipykernel_launcher.py:85: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/Users/timlee/anaconda2/envs/py3/lib/python3.6/site-packages/torch/autograd/__init__.py:93: UserWarning: retain_variables option is deprecated and will be removed in 0.3. Use retain_graph instead.\n",
      "  warnings.warn(\"retain_variables option is deprecated and will be removed in 0.3. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 3.029751, Accuracy: 5/64 (7.8125%)\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.998362, Accuracy: 8/64 (12.5000%)\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.988293, Accuracy: 8/64 (12.5000%)\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.965191, Accuracy: 4/64 (6.2500%)\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.946007, Accuracy: 7/64 (10.9375%)\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.905143, Accuracy: 11/64 (17.1875%)\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.884119, Accuracy: 8/64 (12.5000%)\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.851951, Accuracy: 18/64 (28.1250%)\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.842857, Accuracy: 16/64 (25.0000%)\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.818438, Accuracy: 11/64 (17.1875%)\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.741905, Accuracy: 17/64 (26.5625%)\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 2.710649, Accuracy: 14/64 (21.8750%)\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.685089, Accuracy: 14/64 (21.8750%)\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 2.673091, Accuracy: 15/64 (23.4375%)\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 2.647616, Accuracy: 12/64 (18.7500%)\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 2.665878, Accuracy: 9/64 (14.0625%)\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 2.608279, Accuracy: 17/64 (26.5625%)\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 2.546575, Accuracy: 20/64 (31.2500%)\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 2.526784, Accuracy: 11/64 (17.1875%)\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 2.531688, Accuracy: 18/64 (28.1250%)\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.525115, Accuracy: 19/64 (29.6875%)\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 2.436843, Accuracy: 24/64 (37.5000%)\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 2.476830, Accuracy: 18/64 (28.1250%)\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 2.432953, Accuracy: 23/64 (35.9375%)\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 2.454504, Accuracy: 27/64 (42.1875%)\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 2.453458, Accuracy: 18/64 (28.1250%)\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 2.350724, Accuracy: 22/64 (34.3750%)\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 2.390271, Accuracy: 21/64 (32.8125%)\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 2.350923, Accuracy: 25/64 (39.0625%)\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 2.223141, Accuracy: 30/64 (46.8750%)\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 2.376750, Accuracy: 28/64 (43.7500%)\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 2.239046, Accuracy: 36/64 (56.2500%)\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 2.211625, Accuracy: 38/64 (59.3750%)\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 2.226566, Accuracy: 44/64 (68.7500%)\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 2.238341, Accuracy: 39/64 (60.9375%)\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 2.218534, Accuracy: 41/64 (64.0625%)\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 2.126417, Accuracy: 44/64 (68.7500%)\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 2.171379, Accuracy: 40/64 (62.5000%)\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 2.234912, Accuracy: 41/64 (64.0625%)\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 2.196646, Accuracy: 43/64 (67.1875%)\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.153065, Accuracy: 44/64 (68.7500%)\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 2.220893, Accuracy: 44/64 (68.7500%)\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 2.137853, Accuracy: 45/64 (70.3125%)\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 2.107249, Accuracy: 47/64 (73.4375%)\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 2.122296, Accuracy: 42/64 (65.6250%)\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 2.118581, Accuracy: 45/64 (70.3125%)\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 1.936742, Accuracy: 56/64 (87.5000%)\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 1.957491, Accuracy: 53/64 (82.8125%)\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 2.068275, Accuracy: 46/64 (71.8750%)\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 2.011728, Accuracy: 51/64 (79.6875%)\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 2.041231, Accuracy: 47/64 (73.4375%)\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 1.989562, Accuracy: 50/64 (78.1250%)\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 1.909602, Accuracy: 49/64 (76.5625%)\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 1.948792, Accuracy: 48/64 (75.0000%)\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 1.927809, Accuracy: 48/64 (75.0000%)\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 1.899911, Accuracy: 48/64 (75.0000%)\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 1.929214, Accuracy: 48/64 (75.0000%)\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 1.811609, Accuracy: 52/64 (81.2500%)\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 1.962592, Accuracy: 47/64 (73.4375%)\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 1.751328, Accuracy: 51/64 (79.6875%)\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 2.012541, Accuracy: 45/64 (70.3125%)\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 1.921526, Accuracy: 47/64 (73.4375%)\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 1.903029, Accuracy: 50/64 (78.1250%)\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 1.797055, Accuracy: 51/64 (79.6875%)\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 1.821956, Accuracy: 52/64 (81.2500%)\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 1.682205, Accuracy: 54/64 (84.3750%)\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 1.831255, Accuracy: 51/64 (79.6875%)\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 1.948215, Accuracy: 49/64 (76.5625%)\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 1.978474, Accuracy: 47/64 (73.4375%)\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 1.789337, Accuracy: 54/64 (84.3750%)\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 1.828623, Accuracy: 46/64 (71.8750%)\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 1.798529, Accuracy: 51/64 (79.6875%)\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 1.777987, Accuracy: 50/64 (78.1250%)\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 1.918915, Accuracy: 48/64 (75.0000%)\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 1.719855, Accuracy: 53/64 (82.8125%)\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 1.773376, Accuracy: 53/64 (82.8125%)\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 1.769682, Accuracy: 50/64 (78.1250%)\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 1.719723, Accuracy: 52/64 (81.2500%)\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 1.719431, Accuracy: 51/64 (79.6875%)\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 1.674409, Accuracy: 54/64 (84.3750%)\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.678838, Accuracy: 51/64 (79.6875%)\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 1.682274, Accuracy: 50/64 (78.1250%)\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 1.679456, Accuracy: 53/64 (82.8125%)\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 1.697518, Accuracy: 52/64 (81.2500%)\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 1.616430, Accuracy: 56/64 (87.5000%)\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 1.674750, Accuracy: 53/64 (82.8125%)\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 1.582343, Accuracy: 55/64 (85.9375%)\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 1.613141, Accuracy: 56/64 (87.5000%)\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 1.738777, Accuracy: 53/64 (82.8125%)\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 1.927777, Accuracy: 48/64 (75.0000%)\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 1.742900, Accuracy: 50/64 (78.1250%)\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 1.756195, Accuracy: 48/64 (75.0000%)\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 1.627314, Accuracy: 54/64 (84.3750%)\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 1.570962, Accuracy: 56/64 (87.5000%)\n",
      "\n",
      "Test set: Accuracy: 8350/10000 (83.5000%)\n",
      "\n",
      "directory ./result already exists\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import argparse\n",
    "import pickle\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                    help='input batch size for training (default: 64)')\n",
    "parser.add_argument('--input-dim', type=int, default=28*28, metavar='N',\n",
    "                    help='input dimension size(default: 28 * 28)')\n",
    "parser.add_argument('--output-dim', type=int, default=10, metavar='N',\n",
    "                    help='output dimension size(default: 10)')\n",
    "parser.add_argument('--max-depth', type=int, default=8, metavar='N',\n",
    "                    help='maximum depth of tree(default: 3)')\n",
    "parser.add_argument('--epochs', type=int, default=1, metavar='N',\n",
    "                    help='number of epochs to train (default: 40)')\n",
    "parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n",
    "                    help='learning rate (default: 0.01)')\n",
    "parser.add_argument('--lmbda', type=float, default=0.1, metavar='LR',\n",
    "                    help='temperature rate (default: 0.1)')\n",
    "parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n",
    "                    help='SGD momentum (default: 0.5)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='disables CUDA training')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "try:\n",
    "    os.makedirs('./data')\n",
    "except:\n",
    "    print('directory ./data already exists')\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=False, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])),\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "def save_result(acc):\n",
    "    try:\n",
    "        os.makedirs('./result')\n",
    "    except:\n",
    "        print('directory ./result already exists')\n",
    "    filename = os.path.join('./result/', 'bp_deep.pickle' if args.deep else 'bp.pickle')\n",
    "    f = open(filename,'w')\n",
    "    pickle.dump(acc, f)\n",
    "    f.close()\n",
    "\n",
    "model = SoftDecisionTree(args)\n",
    "\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    model.train_(train_loader, epoch)\n",
    "    model.test_(test_loader, epoch)\n",
    "#save_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for x in model.parameters():\n",
    "#     print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cal_prob() missing 2 required positional arguments: 'x' and 'path_prob'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-ba2068add6dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcal_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: cal_prob() missing 2 required positional arguments: 'x' and 'path_prob'"
     ]
    }
   ],
   "source": [
    "model.root.cal_prob()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata,y = iter(train_loader).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "-0.4242 -0.4242 -0.4242  ...  -0.4242 -0.4242 -0.4242\n",
       "-0.4242 -0.4242 -0.4242  ...  -0.4242 -0.4242 -0.4242\n",
       "-0.4242 -0.4242 -0.4242  ...  -0.4242 -0.4242 -0.4242\n",
       "          ...             â‹±             ...          \n",
       "-0.4242 -0.4242 -0.4242  ...  -0.4242 -0.4242 -0.4242\n",
       "-0.4242 -0.4242 -0.4242  ...  -0.4242 -0.4242 -0.4242\n",
       "-0.4242 -0.4242 -0.4242  ...  -0.4242 -0.4242 -0.4242\n",
       "[torch.FloatTensor of size 64x784]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydata.view(64,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/timlee/anaconda2/envs/py3/lib/python3.6/site-packages/ipykernel_launcher.py:85: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n"
     ]
    }
   ],
   "source": [
    "model.root.cal_prob(Variable(mydata.view(64,-1)), Variable(torch.ones(64,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "    1\n",
       "    1\n",
       "    1\n",
       "    1\n",
       "    1\n",
       "    1\n",
       "    1\n",
       "    1\n",
       "    1\n",
       "    1\n",
       "    1\n",
       "    1\n",
       "    1\n",
       "    1\n",
       "    1\n",
       "    1\n",
       "[torch.FloatTensor of size 16x1]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.path_prob_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
